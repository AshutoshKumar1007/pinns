{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ad6ec",
   "metadata": {},
   "source": [
    "## Deep Ritz Method for Solving PDEs  \n",
    "The Deep Ritz Method is a neural network-based approach for solving partial differential equations (PDEs) by reformulating the problem as a variational problem. This method leverages the expressive power of deep neural networks to approximate the solution of PDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6426aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04108f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_interior(n_samples):\n",
    "    \"\"\"Sample points in the interior of the domain [0, 1] x [0, 1].\"\"\"\n",
    "    x = np.random.uniform(0, 1, (n_samples, 1))\n",
    "    y = np.random.uniform(0, 1, (n_samples, 1))\n",
    "    return torch.tensor(np.hstack((x, y)), dtype=torch.float32).to(device)\n",
    "\n",
    "def sample_boundary(n_samples, require_normals=False):\n",
    "    \"\"\"\n",
    "    Sample points and normal vectorson the boundary of the domain [0, 1] x [0, 1].\n",
    "    Returns:\n",
    "        points: Tensor of shape (n_samples, 2) containing boundary points.\n",
    "        normals: Tensor of shape (n_samples, 2) containing normal vectors at the boundary points.\n",
    "    \"\"\"\n",
    "    n_samples_per_side = n_samples // 4\n",
    "    # Four sides of the square\n",
    "    left = np.stack((np.zeros((n_samples_per_side)), np.random.uniform(0, 1, (n_samples_per_side))), axis=1)\n",
    "    right = np.stack((np.ones((n_samples_per_side)), np.random.uniform(0, 1, (n_samples_per_side))), axis=1)\n",
    "    bottom = np.stack((np.random.uniform(0, 1, (n_samples_per_side)), np.zeros((n_samples_per_side))), axis=1)\n",
    "    top = np.stack((np.random.uniform(0, 1, (n_samples_per_side)), np.ones((n_samples_per_side))), axis=1)\n",
    "    \n",
    "    #stack all boundary points\n",
    "    points = np.vstack((left, right, bottom, top))\n",
    "    normals = None\n",
    "    \n",
    "    if require_normals:\n",
    "        #normal vectors \n",
    "        normals = np.vstack([\n",
    "            np.tile([-1, 0], (n_samples_per_side, 1)),  # left\n",
    "            np.tile([1, 0], (n_samples_per_side, 1)),\n",
    "            np.tile([0, -1], (n_samples_per_side, 1)),  # bottom\n",
    "            np.tile([0, 1], (n_samples_per_side, 1))   # top\n",
    "        ])\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(points, dtype=torch.float32).to(device),\n",
    "        torch.tensor(normals, dtype=torch.float32).to(device) if require_normals else None\n",
    "    )\n",
    "       \n",
    "\n",
    "def generate_samples(n_interior, n_boundary, require_normals=False):\n",
    "    \"\"\"Generate interior and boundary samples.\"\"\"\n",
    "    interior_points = sample_interior(n_interior)\n",
    "    boundary_points, boundary_normals = sample_boundary(n_boundary, require_normals)\n",
    "    return interior_points, boundary_points, boundary_normals\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3744e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d49d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_FCN(nn.Module):\n",
    "    def __init__(self,in__features : int = 2, out_features : int = 1, hidden_dims : list = [8,16,32,32,16,8]):\n",
    "        super(U_FCN,self).__init__()\n",
    "        layers = []\n",
    "        input_dim = in__features\n",
    "        for h_dim in hidden_dims:\n",
    "            # layers.append(nn.BatchNorm1d(input_dim))\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "            # layers.append(nn.Dropout(p=0.2))\n",
    "            input_dim = h_dim\n",
    "        layers.append(nn.Linear(input_dim, out_features))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "    \n",
    "# ---------------biharmonic operator ---------------\n",
    "\n",
    "def biharmonic_operator(u : torch.Tensor,x : torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute Δ²u = Laplacian(Laplacian(u)) using autograd.\n",
    "    x : torch.Tensor (N,2) with required_grad = True\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    u_x = grad_u[:,0:1]\n",
    "    u_y = grad_u[:,1:2]\n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=True)[0][:,0:1]\n",
    "    u_yy = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=True)[0][:,1:2]\n",
    "    lap_u = u_xx + u_yy\n",
    "    \n",
    "    grad_lap_u = torch.autograd.grad(lap_u,x,grad_outputs=torch.ones_like(lap_u),create_graph=True)[0]\n",
    "    lap_u_x = grad_lap_u[:,0:1]\n",
    "    lap_u_y = grad_lap_u[:,1:2]\n",
    "    lap_u_xx = torch.autograd.grad(lap_u_x,x,grad_outputs=torch.ones_like(lap_u_x),create_graph=True)[0][:,0:1]\n",
    "    lap_u_yy = torch.autograd.grad(lap_u_y,x,grad_outputs=torch.ones_like(lap_u_y),create_graph=True)[0][:,1:2]\n",
    "    \n",
    "    return lap_u_xx + lap_u_yy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad695e9",
   "metadata": {},
   "source": [
    "### Penalized Energy Functional (P2)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\lambda(v)\n",
    "= \n",
    "\\frac{1}{2}\\int_{\\Omega} \\lvert D^{2} v \\rvert^{2}\\, dx\n",
    "\\;-\\;\n",
    "\\int_{\\Omega} f\\, v\\, dx\n",
    "\\;-\\;\n",
    "\\int_{\\partial\\Omega} g_{2}\\, \\frac{\\partial v}{\\partial n}\\, ds\n",
    "\\;+\\;\n",
    "\\frac{\\lambda}{2}\\int_{\\partial\\Omega} (v - g_{1})^{2}\\, ds.  \n",
    "\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $D^2 v$ is the Hessian matrix of $v$.  \n",
    "- $|D^2 v|^2 = \\sum_{i,j} \\left( \\frac{\\partial^2 v}{\\partial x_i \\partial x_j} \\right)^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836f11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47e8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian( u : torch.Tensor, x : torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix of u with respect to x.\n",
    "    u : torch.Tensor (N,1)\n",
    "    x : torch.Tensor (N,2) with required_grad = True\n",
    "    return : torch.Tensor (N,2,2)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    u_x = grad_u[:,0:1]\n",
    "    u_y = grad_u[:,1:2]\n",
    "    \n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=True)[0][:,0:1]\n",
    "    u_xy = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=True)[0][:,1:2]\n",
    "    u_yx = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=True)[0][:,0:1]\n",
    "    u_yy = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=True)[0][:,1:2]\n",
    "    \n",
    "    hessian = torch.stack([\n",
    "        torch.cat([u_xx, u_xy], dim=1),\n",
    "        torch.cat([u_yx, u_yy], dim=1)\n",
    "    ], dim=1)  # Shape (N, 2, 2)\n",
    "    \n",
    "    return hessian\n",
    "\n",
    "def compute_normal_derivative(u: torch.Tensor, x: torch.Tensor, normals: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the normal derivative of u at boundary points.\n",
    "    u : torch.Tensor (N,1)\n",
    "    x : torch.Tensor (N,2) with required_grad = True\n",
    "    normals : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    normal_derivative = torch.sum(grad_u * normals, dim=1, keepdim=True)\n",
    "    return normal_derivative\n",
    "\n",
    "def compute_first_normal_derivative(u: torch.Tensor, x: torch.Tensor, normal: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the first normal derivative ∂u/∂n using autograd.\n",
    "    u : torch.Tensor (N,1) with required_grad = True\n",
    "    x : torch.Tensor (N,2)\n",
    "    normal : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True    \n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    du_dn = torch.sum(grad_u * normal, dim=1, keepdim=True) #TODO we could use dot product here\n",
    "    \n",
    "    return du_dn\n",
    "\n",
    "\n",
    "def compute_second_normal_derivative(u: torch.Tensor, x: torch.Tensor, normal: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the second normal derivative ∂²u/∂n² using autograd.\n",
    "    u : torch.Tensor (N,1) with required_grad = True\n",
    "    x : torch.Tensor (N,2)\n",
    "    normal : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    \n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    du_dn = torch.sum(grad_u * normal, dim=1, keepdim=True) #TODO we could use dot product here\n",
    "    \n",
    "    grad_du_dn = torch.autograd.grad(du_dn,x,grad_outputs=torch.ones_like(du_dn),create_graph=True)[0]\n",
    "    d2u_dn2 = torch.sum(grad_du_dn * normal, dim=1, keepdim=True) #similarily\n",
    "    \n",
    "    return d2u_dn2\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ad22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_solution(pts):\n",
    "    \"\"\" \n",
    "    pts : torch.Tensor with shape (N,2)\n",
    "    \"\"\"\n",
    "    if not isinstance(pts,torch.Tensor):\n",
    "        pts = torch.Tensor(pts,dtype = torch.float32)\n",
    "    pts = pts.to(dtype=torch.float32, device=pts.device)\n",
    "    C = 1.0/(2*torch.pi**2)\n",
    "    sol = C * torch.sin(torch.pi*pts[:,0]) * torch.sin(torch.pi*pts[:,1])\n",
    "     \n",
    "    return sol.unsqueeze(-1)\n",
    "\n",
    "def func(pts):\n",
    "    \"\"\" \n",
    "    pts : torch.Tensor with shape (N,2)\n",
    "    \"\"\"\n",
    "    if not isinstance(pts,torch.Tensor):\n",
    "        pts = torch.Tensor(pts,dtype = torch.float32)\n",
    "    pts = pts.to(dtype=torch.float32, device=pts.device)\n",
    "    f =  2.0*torch.pi**2 * torch.sin(torch.pi* pts[:,0]) * torch.sin(torch.pi * pts[:,1])\n",
    "    return f.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def g1(pts):\n",
    "    \"\"\" \n",
    "    g1(pts) = u same as the neural apporximator\n",
    "    \"\"\"\n",
    "    return true_solution(pts)\n",
    "\n",
    "def g2(pts,normals):\n",
    "    \"\"\" \n",
    "    g2(pts,normals) = d2u/dn2\n",
    "    \"\"\"\n",
    "    pts = pts.clone().detach().requires_grad_(True)\n",
    "    normals = normals.clone().detach()\n",
    "    u = true_solution(pts)\n",
    "    # or else we can call the compute_second_normal_derivative function\n",
    "    # d2u_dn2 = normals.unsqueeze(-1) @ compute_hessian(u, pts) @ normals.unsqueeze(-1)\n",
    "    # d2u_dn2 = compute_second_normal_derivative(u, pts, normals)\n",
    "    hessian = compute_hessian(u, pts)  # Shape (N, 2, 2)\n",
    "    d2u_dn2 = torch.einsum('bi,bij,bj->b', normals, hessian, normals).unsqueeze(-1)\n",
    "    return d2u_dn2\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16acd40",
   "metadata": {},
   "source": [
    "### Monte–Carlo Approximation of the Penalized Energy (P2)\n",
    "\n",
    "Let  \n",
    "- $\\{x_i\\}_{i=1}^N \\subset \\Omega$ be interior collocation points,  \n",
    "- $\\{y_j\\}_{j=1}^M \\subset \\partial\\Omega$ be boundary collocation points,  \n",
    "- $|\\Omega|$ the measure of the domain,  \n",
    "- $|\\partial\\Omega|$ the measure of the boundary.\n",
    "\n",
    "Then the Monte–Carlo approximation of $\\mathcal{L}_\\lambda(v)$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\lambda}^{\\text{MC}}(v)\n",
    "=\n",
    "\\frac{1}{2}\\frac{|\\Omega|}{N}\n",
    "\\sum_{i=1}^N \\big|D^2 v(x_i)\\big|^2\n",
    "\\;-\\;\n",
    "\\frac{|\\Omega|}{N}\n",
    "\\sum_{i=1}^N f(x_i)\\, v(x_i)\n",
    "\\;-\\;\n",
    "\\frac{|\\partial\\Omega|}{M}\n",
    "\\sum_{j=1}^M g_2(y_j)\\, \n",
    "\\frac{\\partial v}{\\partial n}(y_j)\n",
    "\\;+\\;\n",
    "\\frac{\\lambda}{2}\\frac{|\\partial\\Omega|}{M}\n",
    "\\sum_{j=1}^M \\left(v(y_j) - g_1(y_j)\\right)^2 .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e45430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interior_loss(model, pts, domain_measure = 1.0):\n",
    "    \"\"\"\n",
    "    Compute the interior loss for the biharmonic equation Δ²u = f.\n",
    "    model : neural network model\n",
    "    pts : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (1,)\n",
    "    \"\"\"       \n",
    "    u = model(pts)\n",
    "    u_true = true_solution(pts)\n",
    "    pinns_loss = F.mse_loss(u, u_true)\n",
    "    hessian = compute_hessian(u, pts)\n",
    "    frobenius_norm_squared = torch.sum(hessian**2, dim=(1, 2))\n",
    "    source_term = func(pts)*u #shape (N,1)\n",
    "    loss_interior = domain_measure * torch.mean(0.5 * frobenius_norm_squared - source_term.squeeze(-1))\n",
    "    return loss_interior,pinns_loss\n",
    "\n",
    "def boundary_loss(model, pts, normals,lambda_dirichlet=1.0, boundary_measure=4.0):\n",
    "    \"\"\"\n",
    "    Compute the boundary loss for the biharmonic equation with boundary conditions.\n",
    "    model : neural network model\n",
    "    pts : torch.Tensor (N,2)\n",
    "    normals : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (1,)\n",
    "    \"\"\"\n",
    "    u = model(pts)\n",
    "    u_true = g1(pts)\n",
    "    _bndr_loss = F.mse_loss(u, u_true)\n",
    "    dirichlet = 0.5 * lambda_dirichlet * boundary_measure  * _bndr_loss\n",
    "    du_dn = compute_first_normal_derivative(u, pts, normals)\n",
    "    neumann = - (g2(pts,normals)* du_dn).mean() * boundary_measure\n",
    "    loss_boundary = dirichlet + neumann\n",
    "    return loss_boundary,_bndr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ddf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, n_interior = 1000, n_boundary = 400):\n",
    "    \"\"\"\n",
    "    Validate the model by computing the L2 error against the true solution.\n",
    "    model : neural network model\n",
    "    n_interior : int, number of interior points for validation\n",
    "    n_boundary : int, number of boundary points for validation\n",
    "    return : float, L2 error\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    interior_pts, boundary_pts, _ = generate_samples(n_interior, n_boundary)\n",
    "    all_pts = torch.cat([interior_pts, boundary_pts], dim=0)\n",
    "    \n",
    "    u_pred = model(all_pts).detach()\n",
    "    u_true = true_solution(all_pts).detach()\n",
    "    \n",
    "    l2_error = torch.sqrt(torch.mean((u_pred - u_true) ** 2)).item()\n",
    "    return l2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def train(\n",
    "    model,\n",
    "    adam_optimizer,\n",
    "    lbfgs_optimizer,\n",
    "    log_var_interior : torch.nn.Parameter,\n",
    "    log_var_boundary : torch.nn.Parameter,\n",
    "    epochs : int = 1000,\n",
    "    n_interior : int = 1000,\n",
    "    n_boundary : int = 400,\n",
    "    switch_epoch : int = 500,\n",
    "    print_interval : int = 100,\n",
    "    save_path : str = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train the model using the Deep Ritz Method for the biharmonic equation.\n",
    "    model : neural network model\n",
    "    adam_optimizer : torch.optim.Optimizer for Adam\n",
    "    lbfgs_optimizer : torch.optim.Optimizer for L-BFGS\n",
    "    log_var_interior : torch.nn.Parameter, log variance for interior loss\n",
    "    log_var_boundary : torch.nn.Parameter, log variance for boundary loss\n",
    "    epochs : int, number of training epochs\n",
    "    n_interior : int, number of interior points per epoch\n",
    "    n_boundary : int, number of boundary points per epoch\n",
    "    switch_epoch : int, epoch to switch from Adam to L-BFGS\n",
    "    print_interval : int, interval for printing training progress\n",
    "    save_path : str, path to save the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    lossses = []\n",
    "    interior_losses = []\n",
    "    boundary_losses = []\n",
    "    device = next(model.parameters()).device\n",
    "    pbar1 = tqdm(range(1,switch_epoch + 1), desc=\"Adam Training\", unit=\"epoch\")\n",
    "    pbar2 = tqdm(range(switch_epoch, epochs + 1), desc=\"L-BFGS Training\", unit=\"epoch\")\n",
    "    for epoch in pbar1:\n",
    "        model.train()\n",
    "        interior_pts, boundary_pts, boundary_normals = generate_samples(n_interior, n_boundary, require_normals=True)\n",
    "        # ensure tensors are on the correct device \n",
    "        interior_pts = interior_pts.to(device)\n",
    "        boundary_pts = boundary_pts.to(device)\n",
    "        boundary_normals = boundary_normals.to(device)\n",
    "        interior_pts.requires_grad = True\n",
    "        boundary_pts.requires_grad = True \n",
    "        \n",
    "        adam_optimizer.zero_grad()\n",
    "        loss_interior = interior_loss(model, interior_pts)\n",
    "        loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "        precision_interior = torch.exp(-log_var_interior)\n",
    "        precision_boundary = torch.exp(-log_var_boundary)\n",
    "        total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "        total_loss.backward()\n",
    "        adam_optimizer.step()\n",
    "        lossses.append(total_loss.item())\n",
    "        interior_losses.append(loss_interior.item())\n",
    "        boundary_losses.append(loss_boundary.item())\n",
    "        if epoch % print_interval == 0 or epoch == 1:\n",
    "            # val_error = validation(model)\n",
    "            pbar1.set_postfix({\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Interior Loss': loss_interior.item(),\n",
    "                'Boundary Loss': loss_boundary.item(),\n",
    "                'precision_interior': precision_interior.item(),\n",
    "                'precision_boundary': precision_boundary.item()\n",
    "            })\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            val_error = validation(model)\n",
    "            print(f\"Validation L2 Error at epoch {epoch}: {val_error}\")\n",
    "            save_path_ = f'{save_path}_Adam_epoch{epoch}.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'adam_optimizer_state_dict': adam_optimizer.state_dict(),\n",
    "                'log_var_interior': log_var_interior,\n",
    "                'log_var_boundary': log_var_boundary\n",
    "            }\n",
    "            ,save_path_\n",
    "            )\n",
    "        \n",
    "    print(\"Switching to L-BFGS Optimizer\")\n",
    "    for epoch in pbar2:\n",
    "        model.train()\n",
    "        interior_pts, boundary_pts, boundary_normals = generate_samples(n_interior, n_boundary, require_normals=True)\n",
    "        # ensure tensors are on the correct device\n",
    "        interior_pts = interior_pts.to(device)\n",
    "        boundary_pts = boundary_pts.to(device)\n",
    "        boundary_normals = boundary_normals.to(device)\n",
    "        interior_pts.requires_grad = True\n",
    "        boundary_pts.requires_grad = True\n",
    "        def closure():\n",
    "            lbfgs_optimizer.zero_grad()\n",
    "            loss_interior = interior_loss(model, interior_pts)\n",
    "            loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "            \n",
    "            precision_interior = torch.exp(-log_var_interior)\n",
    "            precision_boundary = torch.exp(-log_var_boundary)\n",
    "            total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            return total_loss\n",
    "        lbfgs_optimizer.step(closure)\n",
    "        # compute loss for logging\n",
    "        \n",
    "        loss_interior = interior_loss(model, interior_pts)\n",
    "        loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "        precision_interior = torch.exp(-log_var_interior)\n",
    "        precision_boundary = torch.exp(-log_var_boundary)\n",
    "        total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "\n",
    "        lossses.append(total_loss.item())\n",
    "        interior_losses.append(loss_interior.item())\n",
    "        boundary_losses.append(loss_boundary.item())\n",
    "        if epoch % print_interval == 0 or epoch == 1:\n",
    "            # val_error = validation(model)\n",
    "            pbar2.set_postfix({\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Interior Loss': loss_interior.item(),\n",
    "                'Boundary Loss': loss_boundary.item(),\n",
    "                'precision_interior': precision_interior.item(),\n",
    "                'precision_boundary': precision_boundary.item()\n",
    "            })\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            val_error = validation(model)\n",
    "            print(f\"Validation L2 Error at epoch {epoch}: {val_error}\")\n",
    "            save_path_ = f'{save_path}_LBFGS_epoch{epoch}.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'lbfgs_optimizer_state_dict': lbfgs_optimizer.state_dict(),\n",
    "                'log_var_interior': log_var_interior,\n",
    "                'log_var_boundary': log_var_boundary\n",
    "            },save_path_\n",
    "            )\n",
    "               \n",
    "        \n",
    "    return lossses, interior_losses, boundary_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3eb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def train1(\n",
    "    model,\n",
    "    optimizer,\n",
    "    log_var_interior : torch.nn.Parameter,\n",
    "    log_var_boundary : torch.nn.Parameter,\n",
    "    epochs : int = 1000,\n",
    "    n_interior : int = 4000,\n",
    "    n_boundary : int = 1000,\n",
    "    print_interval : int = 100,\n",
    "    save_path : str = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train the model using the Deep Ritz Method for the biharmonic equation.\n",
    "    model : neural network model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    log_var_interior : torch.nn.Parameter, log variance for interior loss\n",
    "    log_var_boundary : torch.nn.Parameter, log variance for boundary loss\n",
    "    epochs : int, number of training epochs\n",
    "    n_interior : int, number of interior points per epoch\n",
    "    n_boundary : int, number of boundary points per epoch\n",
    "    print_interval : int, interval for printing training progress\n",
    "    save_path : str, path to save the trained model\n",
    "    \"\"\"\n",
    "    lossses = []\n",
    "    interior_losses = []\n",
    "    boundary_losses = []\n",
    "    device = next(model.parameters()).device\n",
    "    pbar = tqdm(range(1,epochs + 1), desc=\"Training\", unit=\"epoch\")\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        interior_pts, boundary_pts, boundary_normals = generate_samples(n_interior, n_boundary, require_normals=True)\n",
    "        optimizer.zero_grad()\n",
    "        interior_pts.requires_grad = True\n",
    "        boundary_pts.requires_grad = True\n",
    "        loss_interior,pinns_loss = interior_loss(model, interior_pts)\n",
    "        loss_boundary,_bndr_loss = boundary_loss(model, boundary_pts, boundary_normals, lambda_dirichlet=100.0)\n",
    "        precision_interior = torch.exp(-log_var_interior)\n",
    "        precision_boundary = torch.exp(-log_var_boundary)\n",
    "        #! total loss with uncertainty weighting\n",
    "        total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()   \n",
    "        with torch.no_grad():\n",
    "            log_var_interior.clamp_(0.0, 5.0)\n",
    "            log_var_boundary.clamp_(0.0, 5.0)\n",
    "                 \n",
    "        lossses.append(total_loss.item())\n",
    "        interior_losses.append(loss_interior.item())\n",
    "        boundary_losses.append(loss_boundary.item())\n",
    "        \n",
    "        if epoch % print_interval == 0 or epoch == 1:\n",
    "            # val_error = validation(model)\n",
    "            pbar.set_postfix({\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Interior Loss': loss_interior.item(),\n",
    "                'Boundary Loss': loss_boundary.item(),\n",
    "                'pinns Loss': pinns_loss.item(),\n",
    "                'bndr Loss': _bndr_loss.item(),\n",
    "                'precision_interior': precision_interior.item(),\n",
    "                'precision_boundary': precision_boundary.item()\n",
    "            })\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            val_error = validation(model)\n",
    "            print(f\"Validation L2 Error at epoch {epoch}: {val_error}\")\n",
    "            save_path_ = f'{save_path}_epoch{epoch}.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'log_var_interior': log_var_interior,\n",
    "                'log_var_boundary': log_var_boundary\n",
    "            },save_path_\n",
    "        )\n",
    "    return lossses, interior_losses, boundary_losses\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba804675",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1014210248.py, line 78)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mloss_interior, pinns_loss = interior_loss(model, interior_pts)            loss_boundary, _bndr_loss = boundary_loss(model, boundary_pts, boundary_normals,lambda_dirichlet=100.0)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.makedirs('biharmonic_DRM',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U_FCN(in__features=2,out_features=1,hidden_dims=[16,32,64,128,128,64,32,16]).to(device)\n",
    "adam = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "lbfgs = torch.optim.LBFGS(model.parameters(),lr=0.5,max_iter=20,history_size=10,line_search_fn='strong_wolfe')\n",
    "log_var_interior = torch.nn.Parameter(torch.tensor(0.0,device= device))\n",
    "log_var_boundary = torch.nn.Parameter(torch.tensor(0.0,device= device))\n",
    "adam.add_param_group({'params': [log_var_interior, log_var_boundary],'lr' : 1e-5})\n",
    "# lbfgs.add_param_group({'params': [log_var_interior, log_var_boundary],'lr' : 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23115b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5000 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training:   0%|          | 0/5000 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1600x1 and 2x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m losses, interior_losses, boundary_losses = \u001b[43mtrain1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_var_interior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_var_boundary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_interior\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_boundary\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_interval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbiharmonic_DRM/biharmonic_drm_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain1\u001b[39m\u001b[34m(model, optimizer, log_var_interior, log_var_boundary, epochs, n_interior, n_boundary, print_interval, save_path)\u001b[39m\n\u001b[32m     37\u001b[39m boundary_pts.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m        \n\u001b[32m     38\u001b[39m loss_interior = interior_loss(model, interior_pts)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m loss_boundary = \u001b[43mboundary_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_normals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m precision_interior = torch.exp(-log_var_interior)\n\u001b[32m     42\u001b[39m precision_boundary = torch.exp(-log_var_boundary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mboundary_loss\u001b[39m\u001b[34m(model, pts, normals)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pts.requires_grad:\n\u001b[32m     27\u001b[39m     pts.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m    \n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m u = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m u_true = g1(pts)\n\u001b[32m     30\u001b[39m loss_bc1 = F.mse_loss(u, u_true)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1750\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1753\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1754\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mU_FCN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1750\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1753\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1754\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1750\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1753\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1754\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1600x1 and 2x16)"
     ]
    }
   ],
   "source": [
    "losses, interior_losses, boundary_losses = train(\n",
    "    model,\n",
    "    adam,\n",
    "    lbfgs,\n",
    "    log_var_interior,\n",
    "    log_var_boundary,\n",
    "    epochs = 1000,\n",
    "    switch_epoch= 500,\n",
    "    n_interior = 8000,\n",
    "    n_boundary = 4000,\n",
    "    print_interval = 50,\n",
    "    save_path = 'biharmonic_DRM/biharmonic_model'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a362925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [00:42<00:00, 11.79epoch/s, Total Loss=-0.0345, Interior Loss=-0.178, Boundary Loss=0.112, pinns Loss=0.000243, bndr Loss=0.000562, precision_interior=1, precision_boundary=0.998]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [00:42<00:00, 11.79epoch/s, Total Loss=-0.0345, Interior Loss=-0.178, Boundary Loss=0.112, pinns Loss=0.000243, bndr Loss=0.000562, precision_interior=1, precision_boundary=0.998]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.018510593101382256\n",
      "Switching to L-BFGS Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [00:42<00:00, 11.79epoch/s, Total Loss=-0.0345, Interior Loss=-0.178, Boundary Loss=0.112, pinns Loss=0.000243, bndr Loss=0.000562, precision_interior=1, precision_boundary=0.998]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.018510593101382256\n",
      "Switching to L-BFGS Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L-BFGS Training:   0%|          | 0/501 [00:44<?, ?epoch/s, Total Loss=-0.0661, Interior Loss=-0.213, Boundary Loss=0.0846, pinns Loss=0.000169, bndr Loss=0.000423]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [00:42<00:00, 11.79epoch/s, Total Loss=-0.0345, Interior Loss=-0.178, Boundary Loss=0.112, pinns Loss=0.000243, bndr Loss=0.000562, precision_interior=1, precision_boundary=0.998]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.018510593101382256\n",
      "Switching to L-BFGS Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L-BFGS Training:   0%|          | 0/501 [00:44<?, ?epoch/s, Total Loss=-0.0661, Interior Loss=-0.213, Boundary Loss=0.0846, pinns Loss=0.000169, bndr Loss=0.000423]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.01555402297526598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [00:42<00:00, 11.79epoch/s, Total Loss=-0.0345, Interior Loss=-0.178, Boundary Loss=0.112, pinns Loss=0.000243, bndr Loss=0.000562, precision_interior=1, precision_boundary=0.998]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.018510593101382256\n",
      "Switching to L-BFGS Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L-BFGS Training:   0%|          | 0/501 [00:44<?, ?epoch/s, Total Loss=-0.0661, Interior Loss=-0.213, Boundary Loss=0.0846, pinns Loss=0.000169, bndr Loss=0.000423]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.01555402297526598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L-BFGS Training:   7%|▋         | 35/501 [01:59<26:35,  3.42s/epoch, Total Loss=-0.0661, Interior Loss=-0.213, Boundary Loss=0.0846, pinns Loss=0.000169, bndr Loss=0.000423]]\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:   0%|          | 0/500 [00:00<?, ?epoch/s]c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Adam Training:  20%|██        | 101/500 [00:07<00:27, 14.32epoch/s, Total Loss=2.55, Interior Loss=-1.46, Boundary Loss=6.57, pinns Loss=0.0259, bndr Loss=0.0328, precision_interior=1, precision_boundary=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 100: 0.1668606698513031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  41%|████      | 203/500 [00:15<00:23, 12.51epoch/s, Total Loss=1.22, Interior Loss=-1.04, Boundary Loss=3.48, pinns Loss=0.0126, bndr Loss=0.0174, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 200: 0.1182311549782753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  60%|██████    | 301/500 [00:23<00:16, 12.18epoch/s, Total Loss=0.303, Interior Loss=-0.635, Boundary Loss=1.25, pinns Loss=0.00354, bndr Loss=0.00623, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 300: 0.06529223173856735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training:  80%|████████  | 401/500 [00:33<00:08, 11.21epoch/s, Total Loss=-0.00044, Interior Loss=-0.303, Boundary Loss=0.305, pinns Loss=0.00055, bndr Loss=0.00152, precision_interior=1, precision_boundary=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 400: 0.02858593687415123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [00:42<00:00, 11.79epoch/s, Total Loss=-0.0345, Interior Loss=-0.178, Boundary Loss=0.112, pinns Loss=0.000243, bndr Loss=0.000562, precision_interior=1, precision_boundary=0.998]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.018510593101382256\n",
      "Switching to L-BFGS Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L-BFGS Training:   0%|          | 0/501 [00:44<?, ?epoch/s, Total Loss=-0.0661, Interior Loss=-0.213, Boundary Loss=0.0846, pinns Loss=0.000169, bndr Loss=0.000423]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.01555402297526598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L-BFGS Training:   7%|▋         | 35/501 [01:59<26:35,  3.42s/epoch, Total Loss=-0.0661, Interior Loss=-0.213, Boundary Loss=0.0846, pinns Loss=0.000169, bndr Loss=0.000423]]\u001b[A\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m losses, interior_losses, boundary_losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43madam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_var_interior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_var_boundary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mswitch_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_interior\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_boundary\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_interval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbiharmonic_DRM/biharmonic_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, adam_optimizer, lbfgs_optimizer, log_var_interior, log_var_boundary, epochs, n_interior, n_boundary, switch_epoch, print_interval, save_path)\u001b[39m\n\u001b[32m    101\u001b[39m     total_loss.backward()\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mlbfgs_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# compute loss for logging\u001b[39;00m\n\u001b[32m    106\u001b[39m loss_interior,pinns_loss = interior_loss(model, interior_pts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\optim\\lbfgs.py:444\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj_func\u001b[39m(x, t, d):\n\u001b[32m    442\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._directional_evaluate(closure, x, t, d)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     loss, flat_grad, t, ls_func_evals = \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28mself\u001b[39m._add_grad(t, d)\n\u001b[32m    448\u001b[39m opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\optim\\lbfgs.py:48\u001b[39m, in \u001b[36m_strong_wolfe\u001b[39m\u001b[34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[39m\n\u001b[32m     46\u001b[39m g = g.clone(memory_format=torch.contiguous_format)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m f_new, g_new = \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m ls_func_evals = \u001b[32m1\u001b[39m\n\u001b[32m     50\u001b[39m gtd_new = g_new.dot(d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\optim\\lbfgs.py:442\u001b[39m, in \u001b[36mLBFGS.step.<locals>.obj_func\u001b[39m\u001b[34m(x, t, d)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj_func\u001b[39m(x, t, d):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\optim\\lbfgs.py:296\u001b[39m, in \u001b[36mLBFGS._directional_evaluate\u001b[39m\u001b[34m(self, closure, x, t, d)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_grad(t, d)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    297\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_param(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mtrain.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m():\n\u001b[32m     93\u001b[39m     lbfgs_optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     loss_interior,_ = \u001b[43minterior_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterior_pts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     loss_boundary,_ = boundary_loss(model, boundary_pts, boundary_normals,lambda_dirichlet=\u001b[32m100.0\u001b[39m)\n\u001b[32m     97\u001b[39m     precision_interior = torch.exp(-log_var_interior)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36minterior_loss\u001b[39m\u001b[34m(model, pts, domain_measure)\u001b[39m\n\u001b[32m      9\u001b[39m u_true = true_solution(pts)\n\u001b[32m     10\u001b[39m pinns_loss = F.mse_loss(u, u_true)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m hessian = \u001b[43mcompute_hessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m frobenius_norm_squared = torch.sum(hessian**\u001b[32m2\u001b[39m, dim=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m     13\u001b[39m source_term = func(pts)*u \u001b[38;5;66;03m#shape (N,1)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcompute_hessian\u001b[39m\u001b[34m(u, x)\u001b[39m\n\u001b[32m     15\u001b[39m u_xy = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m][:,\u001b[32m1\u001b[39m:\u001b[32m2\u001b[39m]\n\u001b[32m     16\u001b[39m u_yx = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m][:,\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m u_yy = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][:,\u001b[32m1\u001b[39m:\u001b[32m2\u001b[39m]\n\u001b[32m     19\u001b[39m hessian = torch.stack([\n\u001b[32m     20\u001b[39m     torch.cat([u_xx, u_xy], dim=\u001b[32m1\u001b[39m),\n\u001b[32m     21\u001b[39m     torch.cat([u_yx, u_yy], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m ], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Shape (N, 2, 2)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hessian\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\venvs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Plotting the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label='Total Loss')\n",
    "plt.plot(interior_losses, label='Interior Loss')\n",
    "plt.plot(boundary_losses, label='Boundary Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('biharmonic_DRM/training_losses.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
