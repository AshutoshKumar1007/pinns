{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf4965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ad6ec",
   "metadata": {},
   "source": [
    "## Deep Ritz Method for Solving PDEs  \n",
    "The Deep Ritz Method is a neural network-based approach for solving partial differential equations (PDEs) by reformulating the problem as a variational problem. This method leverages the expressive power of deep neural networks to approximate the solution of PDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6426aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04108f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_interior(n_samples):\n",
    "    \"\"\"Sample points in the interior of the domain [0, 1] x [0, 1].\"\"\"\n",
    "    x = np.random.uniform(0, 1, (n_samples, 1))\n",
    "    y = np.random.uniform(0, 1, (n_samples, 1))\n",
    "    return torch.tensor(np.hstack((x, y)), dtype=torch.float32).to(device)\n",
    "\n",
    "def sample_boundary(n_samples, require_normals=False):\n",
    "    \"\"\"\n",
    "    Sample points and normal vectorson the boundary of the domain [0, 1] x [0, 1].\n",
    "    Returns:\n",
    "        points: Tensor of shape (n_samples, 2) containing boundary points.\n",
    "        normals: Tensor of shape (n_samples, 2) containing normal vectors at the boundary points.\n",
    "    \"\"\"\n",
    "    n_samples_per_side = n_samples // 4\n",
    "    # Four sides of the square\n",
    "    left = np.stack((np.zeros((n_samples_per_side)), np.random.uniform(0, 1, (n_samples_per_side))), axis=1)\n",
    "    right = np.stack((np.ones((n_samples_per_side)), np.random.uniform(0, 1, (n_samples_per_side))), axis=1)\n",
    "    bottom = np.stack((np.random.uniform(0, 1, (n_samples_per_side)), np.zeros((n_samples_per_side))), axis=1)\n",
    "    top = np.stack((np.random.uniform(0, 1, (n_samples_per_side)), np.ones((n_samples_per_side))), axis=1)\n",
    "    \n",
    "    #stack all boundary points\n",
    "    points = np.vstack((left, right, bottom, top))\n",
    "    normals = None\n",
    "    \n",
    "    if require_normals:\n",
    "        #normal vectors \n",
    "        normals = np.vstack([\n",
    "            np.tile([-1, 0], (n_samples_per_side, 1)),  # left\n",
    "            np.tile([1, 0], (n_samples_per_side, 1)),\n",
    "            np.tile([0, -1], (n_samples_per_side, 1)),  # bottom\n",
    "            np.tile([0, 1], (n_samples_per_side, 1))   # top\n",
    "        ])\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(points, dtype=torch.float32).to(device),\n",
    "        torch.tensor(normals, dtype=torch.float32).to(device) if require_normals else None\n",
    "    )\n",
    "       \n",
    "\n",
    "def generate_samples(n_interior, n_boundary, require_normals=False):\n",
    "    \"\"\"Generate interior and boundary samples.\"\"\"\n",
    "    interior_points = sample_interior(n_interior)\n",
    "    boundary_points, boundary_normals = sample_boundary(n_boundary, require_normals)\n",
    "    return interior_points, boundary_points, boundary_normals\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ffd485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([16, 2]), torch.Size([16, 2]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c = generate_samples(10, 16, require_normals=True)\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3744e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47d49d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_FCN(nn.Module):\n",
    "    def __init__(self,in__features : int = 2, out_features : int = 1, hidden_dims : list = [8,16,32,32,16,8]):\n",
    "        super(U_FCN,self).__init__()\n",
    "        layers = []\n",
    "        input_dim = in__features\n",
    "        for h_dim in hidden_dims:\n",
    "            # layers.append(nn.BatchNorm1d(input_dim))\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "            # layers.append(nn.Dropout(p=0.2))\n",
    "            input_dim = h_dim\n",
    "        layers.append(nn.Linear(input_dim, out_features))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "    \n",
    "# ---------------biharmonic operator ---------------\n",
    "\n",
    "def biharmonic_operator(u : torch.Tensor,x : torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute Δ²u = Laplacian(Laplacian(u)) using autograd.\n",
    "    x : torch.Tensor (N,2) with required_grad = True\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    u_x = grad_u[:,0:1]\n",
    "    u_y = grad_u[:,1:2]\n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=True)[0][:,0:1]\n",
    "    u_yy = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=True)[0][:,1:2]\n",
    "    lap_u = u_xx + u_yy\n",
    "    \n",
    "    grad_lap_u = torch.autograd.grad(lap_u,x,grad_outputs=torch.ones_like(lap_u),create_graph=True)[0]\n",
    "    lap_u_x = grad_lap_u[:,0:1]\n",
    "    lap_u_y = grad_lap_u[:,1:2]\n",
    "    lap_u_xx = torch.autograd.grad(lap_u_x,x,grad_outputs=torch.ones_like(lap_u_x),create_graph=True)[0][:,0:1]\n",
    "    lap_u_yy = torch.autograd.grad(lap_u_y,x,grad_outputs=torch.ones_like(lap_u_y),create_graph=True)[0][:,1:2]\n",
    "    \n",
    "    return lap_u_xx + lap_u_yy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad695e9",
   "metadata": {},
   "source": [
    "### Penalized Energy Functional (P2)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\lambda(v)\n",
    "= \n",
    "\\frac{1}{2}\\int_{\\Omega} \\lvert D^{2} v \\rvert^{2}\\, dx\n",
    "\\;-\\;\n",
    "\\int_{\\Omega} f\\, v\\, dx\n",
    "\\;-\\;\n",
    "\\int_{\\partial\\Omega} g_{2}\\, \\frac{\\partial v}{\\partial n}\\, ds\n",
    "\\;+\\;\n",
    "\\frac{\\lambda}{2}\\int_{\\partial\\Omega} (v - g_{1})^{2}\\, ds.  \n",
    "\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $D^2 v$ is the Hessian matrix of $v$.  \n",
    "- $|D^2 v|^2 = \\sum_{i,j} \\left( \\frac{\\partial^2 v}{\\partial x_i \\partial x_j} \\right)^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836f11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e47e8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian( u : torch.Tensor, x : torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix of u with respect to x.\n",
    "    u : torch.Tensor (N,1)\n",
    "    x : torch.Tensor (N,2) with required_grad = True\n",
    "    return : torch.Tensor (N,2,2)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    u_x = grad_u[:,0:1]\n",
    "    u_y = grad_u[:,1:2]\n",
    "    \n",
    "    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=True)[0][:,0:1]\n",
    "    u_xy = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),create_graph=True)[0][:,1:2]\n",
    "    u_yx = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=True)[0][:,0:1]\n",
    "    u_yy = torch.autograd.grad(u_y,x,grad_outputs=torch.ones_like(u_y),create_graph=True)[0][:,1:2]\n",
    "    \n",
    "    hessian = torch.stack([\n",
    "        torch.cat([u_xx, u_xy], dim=1),\n",
    "        torch.cat([u_yx, u_yy], dim=1)\n",
    "    ], dim=1)  # Shape (N, 2, 2)\n",
    "    \n",
    "    return hessian\n",
    "\n",
    "def compute_normal_derivative(u: torch.Tensor, x: torch.Tensor, normals: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the normal derivative of u at boundary points.\n",
    "    u : torch.Tensor (N,1)\n",
    "    x : torch.Tensor (N,2) with required_grad = True\n",
    "    normals : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    normal_derivative = torch.sum(grad_u * normals, dim=1, keepdim=True)\n",
    "    return normal_derivative\n",
    "\n",
    "def compute_first_normal_derivative(u: torch.Tensor, x: torch.Tensor, normal: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the first normal derivative ∂u/∂n using autograd.\n",
    "    u : torch.Tensor (N,1) with required_grad = True\n",
    "    x : torch.Tensor (N,2)\n",
    "    normal : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    \n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    du_dn = torch.sum(grad_u * normal, dim=1, keepdim=True) #TODO we could use dot product here\n",
    "    \n",
    "    return du_dn\n",
    "\n",
    "\n",
    "def compute_second_normal_derivative(u: torch.Tensor, x: torch.Tensor, normal: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the second normal derivative ∂²u/∂n² using autograd.\n",
    "    u : torch.Tensor (N,1) with required_grad = True\n",
    "    x : torch.Tensor (N,2)\n",
    "    normal : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (N,1)\n",
    "    \"\"\"\n",
    "    if not x.requires_grad:\n",
    "        x.requires_grad = True\n",
    "    \n",
    "    grad_u = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),create_graph=True)[0]\n",
    "    du_dn = torch.sum(grad_u * normal, dim=1, keepdim=True) #TODO we could use dot product here\n",
    "    \n",
    "    grad_du_dn = torch.autograd.grad(du_dn,x,grad_outputs=torch.ones_like(du_dn),create_graph=True)[0]\n",
    "    d2u_dn2 = torch.sum(grad_du_dn * normal, dim=1, keepdim=True) #similarily\n",
    "    \n",
    "    return d2u_dn2\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1ad22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_solution(pts):\n",
    "    \"\"\" \n",
    "    pts : torch.Tensor with shape (N,2)\n",
    "    \"\"\"\n",
    "    if not isinstance(pts,torch.Tensor):\n",
    "        pts = torch.Tensor(pts,dtype = torch.float32)\n",
    "    pts = pts.to(dtype=torch.float32, device=pts.device)\n",
    "    C = 1.0/(2*torch.pi**2)\n",
    "    sol = C * torch.sin(torch.pi*pts[:,0]) * torch.sin(torch.pi*pts[:,1])\n",
    "     \n",
    "    return sol.unsqueeze(-1)\n",
    "\n",
    "def func(pts):\n",
    "    \"\"\" \n",
    "    pts : torch.Tensor with shape (N,2)\n",
    "    \"\"\"\n",
    "    if not isinstance(pts,torch.Tensor):\n",
    "        pts = torch.Tensor(pts,dtype = torch.float32)\n",
    "    pts = pts.to(dtype=torch.float32, device=pts.device)\n",
    "    f =  2.0*torch.pi**2 * torch.sin(torch.pi* pts[:,0]) * torch.sin(torch.pi * pts[:,1])\n",
    "    return f.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def g1(pts):\n",
    "    \"\"\" \n",
    "    g1(pts) = u same as the neural apporximator\n",
    "    \"\"\"\n",
    "    return true_solution(pts)\n",
    "\n",
    "def g2(pts,normals):\n",
    "    \"\"\" \n",
    "    g2(pts,normals) = d2u/dn2\n",
    "    \"\"\"\n",
    "    u = true_solution(pts)\n",
    "    # or else we can call the compute_second_normal_derivative function\n",
    "    # d2u_dn2 = normals.unsqueeze(-1) @ compute_hessian(u, pts) @ normals.unsqueeze(-1)\n",
    "    # d2u_dn2 = compute_second_normal_derivative(u, pts, normals)\n",
    "    hessian = compute_hessian(u, pts)  # Shape (N, 2, 2)\n",
    "    d2u_dn2 = torch.einsum('bi,bij,bj->b', normals, hessian, normals)\n",
    "    return d2u_dn2.squeeze(-1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16acd40",
   "metadata": {},
   "source": [
    "### Monte–Carlo Approximation of the Penalized Energy (P2)\n",
    "\n",
    "Let  \n",
    "- $\\{x_i\\}_{i=1}^N \\subset \\Omega$ be interior collocation points,  \n",
    "- $\\{y_j\\}_{j=1}^M \\subset \\partial\\Omega$ be boundary collocation points,  \n",
    "- $|\\Omega|$ the measure of the domain,  \n",
    "- $|\\partial\\Omega|$ the measure of the boundary.\n",
    "\n",
    "Then the Monte–Carlo approximation of $\\mathcal{L}_\\lambda(v)$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\lambda}^{\\text{MC}}(v)\n",
    "=\n",
    "\\frac{1}{2}\\frac{|\\Omega|}{N}\n",
    "\\sum_{i=1}^N \\big|D^2 v(x_i)\\big|^2\n",
    "\\;-\\;\n",
    "\\frac{|\\Omega|}{N}\n",
    "\\sum_{i=1}^N f(x_i)\\, v(x_i)\n",
    "\\;-\\;\n",
    "\\frac{|\\partial\\Omega|}{M}\n",
    "\\sum_{j=1}^M g_2(y_j)\\, \n",
    "\\frac{\\partial v}{\\partial n}(y_j)\n",
    "\\;+\\;\n",
    "\\frac{\\lambda}{2}\\frac{|\\partial\\Omega|}{M}\n",
    "\\sum_{j=1}^M \\left(v(y_j) - g_1(y_j)\\right)^2 .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e45430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interior_loss(model, pts):\n",
    "    \"\"\"\n",
    "    Compute the interior loss for the biharmonic equation Δ²u = f.\n",
    "    model : neural network model\n",
    "    pts : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (1,)\n",
    "    \"\"\"\n",
    "    pts = pts.clone().detach().requires_grad_(True)        \n",
    "    u = model(pts)\n",
    "    hessian = compute_hessian(u, pts)\n",
    "    frobenius_norm_squared = torch.sum(hessian**2, dim=(1, 2))\n",
    "    source_term = func(pts)*u #shape (N,1)\n",
    "    loss_interior = torch.mean(0.5 * frobenius_norm_squared - source_term.squeeze(-1))\n",
    "    return loss_interior\n",
    "\n",
    "def boundary_loss(model, pts, normals):\n",
    "    \"\"\"\n",
    "    Compute the boundary loss for the biharmonic equation with boundary conditions.\n",
    "    model : neural network model\n",
    "    pts : torch.Tensor (N,2)\n",
    "    normals : torch.Tensor (N,2)\n",
    "    return : torch.Tensor (1,)\n",
    "    \"\"\"\n",
    "    pts = pts.clone().detach().requires_grad_(True)  \n",
    "    u = model(pts)\n",
    "    u_true = g1(pts)\n",
    "    loss_bc1 = F.mse_loss(u, u_true)\n",
    "    \n",
    "    neumann_term = g2(pts, normals)*compute_first_normal_derivative(u, pts, normals)\n",
    "    loss_bc2 = torch.mean(neumann_term)\n",
    "    loss_boundary = loss_bc1 - loss_bc2\n",
    "    return loss_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71ddf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, n_interior = 1000, n_boundary = 400):\n",
    "    \"\"\"\n",
    "    Validate the model by computing the L2 error against the true solution.\n",
    "    model : neural network model\n",
    "    n_interior : int, number of interior points for validation\n",
    "    n_boundary : int, number of boundary points for validation\n",
    "    return : float, L2 error\n",
    "    \"\"\"\n",
    "    interior_pts, boundary_pts, _ = generate_samples(n_interior, n_boundary)\n",
    "    all_pts = torch.cat([interior_pts, boundary_pts], dim=0)\n",
    "    \n",
    "    u_pred = model(all_pts).detach()\n",
    "    u_true = true_solution(all_pts).detach()\n",
    "    \n",
    "    l2_error = torch.sqrt(torch.mean((u_pred - u_true) ** 2)).item()\n",
    "    return l2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c8bede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def train(\n",
    "    model,\n",
    "    adam_optimizer,\n",
    "    lbfgs_optimizer,\n",
    "    log_var_interior : torch.nn.Parameter,\n",
    "    log_var_boundary : torch.nn.Parameter,\n",
    "    epochs : int = 1000,\n",
    "    n_interior : int = 1000,\n",
    "    n_boundary : int = 400,\n",
    "    switch_epoch : int = 500,\n",
    "    print_interval : int = 100,\n",
    "    save_path : str = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train the model using the Deep Ritz Method for the biharmonic equation.\n",
    "    model : neural network model\n",
    "    adam_optimizer : torch.optim.Optimizer for Adam\n",
    "    lbfgs_optimizer : torch.optim.Optimizer for L-BFGS\n",
    "    log_var_interior : torch.nn.Parameter, log variance for interior loss\n",
    "    log_var_boundary : torch.nn.Parameter, log variance for boundary loss\n",
    "    epochs : int, number of training epochs\n",
    "    n_interior : int, number of interior points per epoch\n",
    "    n_boundary : int, number of boundary points per epoch\n",
    "    switch_epoch : int, epoch to switch from Adam to L-BFGS\n",
    "    print_interval : int, interval for printing training progress\n",
    "    save_path : str, path to save the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    lossses = []\n",
    "    interior_losses = []\n",
    "    boundary_losses = []\n",
    "    device = next(model.parameters()).device\n",
    "    pbar1 = tqdm(range(1,switch_epoch + 1), desc=\"Adam Training\", unit=\"epoch\")\n",
    "    pbar2 = tqdm(range(switch_epoch, epochs + 1), desc=\"L-BFGS Training\", unit=\"epoch\")\n",
    "    for epoch in pbar1:\n",
    "        model.train()\n",
    "        interior_pts, boundary_pts, boundary_normals = generate_samples(n_interior, n_boundary, require_normals=True)\n",
    "        # ensure tensors are on the correct device \n",
    "        interior_pts = interior_pts.to(device)\n",
    "        boundary_pts = boundary_pts.to(device)\n",
    "        boundary_normals = boundary_normals.to(device)\n",
    "        interior_pts.requires_grad = True\n",
    "        boundary_pts.requires_grad = True \n",
    "        \n",
    "        adam_optimizer.zero_grad()\n",
    "        loss_interior = interior_loss(model, interior_pts)\n",
    "        loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "        precision_interior = torch.exp(-log_var_interior)\n",
    "        precision_boundary = torch.exp(-log_var_boundary)\n",
    "        total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "        total_loss.backward()\n",
    "        adam_optimizer.step()\n",
    "        lossses.append(total_loss.item())\n",
    "        interior_losses.append(loss_interior.item())\n",
    "        boundary_losses.append(loss_boundary.item())\n",
    "        if epoch % print_interval == 0 or epoch == 1:\n",
    "            # val_error = validation(model)\n",
    "            pbar1.set_postfix({\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Interior Loss': loss_interior.item(),\n",
    "                'Boundary Loss': loss_boundary.item(),\n",
    "                'precision_interior': precision_interior.item(),\n",
    "                'precision_boundary': precision_boundary.item()\n",
    "            })\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            val_error = validation(model)\n",
    "            print(f\"Validation L2 Error at epoch {epoch}: {val_error}\")\n",
    "            save_path_ = f'{save_path}_Adam_epoch{epoch}.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'adam_optimizer_state_dict': adam_optimizer.state_dict(),\n",
    "                'log_var_interior': log_var_interior,\n",
    "                'log_var_boundary': log_var_boundary\n",
    "            }\n",
    "            ,save_path_\n",
    "            )\n",
    "        \n",
    "    print(\"Switching to L-BFGS Optimizer\")\n",
    "    for epoch in pbar2:\n",
    "        model.train()\n",
    "        interior_pts, boundary_pts, boundary_normals = generate_samples(n_interior, n_boundary, require_normals=True)\n",
    "        # ensure tensors are on the correct device\n",
    "        interior_pts = interior_pts.to(device)\n",
    "        boundary_pts = boundary_pts.to(device)\n",
    "        boundary_normals = boundary_normals.to(device)\n",
    "        interior_pts.requires_grad = True\n",
    "        boundary_pts.requires_grad = True\n",
    "        def closure():\n",
    "            lbfgs_optimizer.zero_grad()\n",
    "            loss_interior = interior_loss(model, interior_pts)\n",
    "            loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "            \n",
    "            precision_interior = torch.exp(-log_var_interior)\n",
    "            precision_boundary = torch.exp(-log_var_boundary)\n",
    "            total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            return total_loss\n",
    "        lbfgs_optimizer.step(closure)\n",
    "        # compute loss for logging\n",
    "        \n",
    "        loss_interior = interior_loss(model, interior_pts)\n",
    "        loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "        precision_interior = torch.exp(-log_var_interior)\n",
    "        precision_boundary = torch.exp(-log_var_boundary)\n",
    "        total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "\n",
    "        lossses.append(total_loss.item())\n",
    "        interior_losses.append(loss_interior.item())\n",
    "        boundary_losses.append(loss_boundary.item())\n",
    "        if epoch % print_interval == 0 or epoch == 1:\n",
    "            # val_error = validation(model)\n",
    "            pbar2.set_postfix({\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Interior Loss': loss_interior.item(),\n",
    "                'Boundary Loss': loss_boundary.item(),\n",
    "                'precision_interior': precision_interior.item(),\n",
    "                'precision_boundary': precision_boundary.item()\n",
    "            })\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            val_error = validation(model)\n",
    "            print(f\"Validation L2 Error at epoch {epoch}: {val_error}\")\n",
    "            save_path_ = f'{save_path}_LBFGS_epoch{epoch}.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'lbfgs_optimizer_state_dict': lbfgs_optimizer.state_dict(),\n",
    "                'log_var_interior': log_var_interior,\n",
    "                'log_var_boundary': log_var_boundary\n",
    "            },save_path_\n",
    "            )\n",
    "               \n",
    "        \n",
    "    return lossses, interior_losses, boundary_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b3eb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def train1(\n",
    "    model,\n",
    "    optimizer,\n",
    "    log_var_interior : torch.nn.Parameter,\n",
    "    log_var_boundary : torch.nn.Parameter,\n",
    "    epochs : int = 1000,\n",
    "    n_interior : int = 4000,\n",
    "    n_boundary : int = 1000,\n",
    "    print_interval : int = 100,\n",
    "    save_path : str = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train the model using the Deep Ritz Method for the biharmonic equation.\n",
    "    model : neural network model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    log_var_interior : torch.nn.Parameter, log variance for interior loss\n",
    "    log_var_boundary : torch.nn.Parameter, log variance for boundary loss\n",
    "    epochs : int, number of training epochs\n",
    "    n_interior : int, number of interior points per epoch\n",
    "    n_boundary : int, number of boundary points per epoch\n",
    "    print_interval : int, interval for printing training progress\n",
    "    save_path : str, path to save the trained model\n",
    "    \"\"\"\n",
    "    lossses = []\n",
    "    interior_losses = []\n",
    "    boundary_losses = []\n",
    "    device = next(model.parameters()).device\n",
    "    pbar = tqdm(range(1,epochs + 1), desc=\"Training\", unit=\"epoch\")\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        interior_pts, boundary_pts, boundary_normals = generate_samples(n_interior, n_boundary, require_normals=True)\n",
    "        def closer():\n",
    "            optimizer.zero_grad()\n",
    "            interior_pts.requires_grad = True\n",
    "            boundary_pts.requires_grad = True\n",
    "            loss_interior = interior_loss(model, interior_pts)\n",
    "            loss_boundary = boundary_loss(model, boundary_pts, boundary_normals)\n",
    "            precision_interior = torch.exp(-log_var_interior)\n",
    "            precision_boundary = torch.exp(-log_var_boundary)\n",
    "            #! total loss with uncertainty weighting\n",
    "            total_loss = 0.5*(precision_interior * loss_interior + log_var_interior + precision_boundary * loss_boundary + log_var_boundary)\n",
    "            total_loss.backward()\n",
    "            return total_loss,loss_interior, loss_boundary, precision_interior, precision_boundary\n",
    "        optimizer.step(closer)\n",
    "        total_loss,loss_interior, loss_boundary, precision_interior, precision_boundary = closer()\n",
    "        with torch.no_grad():\n",
    "            log_var_interior.clamp_(0.0, 5.0)\n",
    "            log_var_boundary.clamp_(0.0, 5.0)\n",
    "        \n",
    "        lossses.append(total_loss.item())\n",
    "        interior_losses.append(loss_interior.item())\n",
    "        boundary_losses.append(loss_boundary.item())\n",
    "        \n",
    "        if epoch % print_interval == 0 or epoch == 1:\n",
    "            # val_error = validation(model)\n",
    "            pbar.set_postfix({\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Interior Loss': loss_interior.item(),\n",
    "                'Boundary Loss': loss_boundary.item(),\n",
    "                'precision_interior': precision_interior.item(),\n",
    "                'precision_boundary': precision_boundary.item()\n",
    "            })\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            val_error = validation(model)\n",
    "            print(f\"Validation L2 Error at epoch {epoch}: {val_error}\")\n",
    "            save_path_ = f'{save_path}_epoch{epoch}.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'log_var_interior': log_var_interior,\n",
    "                'log_var_boundary': log_var_boundary\n",
    "            },save_path_\n",
    "        )\n",
    "        return lossses, interior_losses, boundary_losses\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba804675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.makedirs('biharmonic_DRM',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3084545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U_FCN(in__features=2,out_features=1,hidden_dims=[16,32,64,128,128,64,32,16]).to(device)\n",
    "adam = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "lbfgs = torch.optim.LBFGS(model.parameters(),lr=1.0,max_iter=20,history_size=25,line_search_fn='strong_wolfe')\n",
    "log_var_interior = torch.nn.Parameter(torch.tensor(0.0,device= device))\n",
    "log_var_boundary = torch.nn.Parameter(torch.tensor(0.0,device= device))\n",
    "adam.add_param_group({'params': [log_var_interior, log_var_boundary],'lr' : 1e-5})\n",
    "# lbfgs.add_param_group({'params': [log_var_interior, log_var_boundary],'lr' : 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23115b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adam Training: 100%|██████████| 500/500 [01:14<00:00,  6.67epoch/s, Total Loss=-2.08, Interior Loss=-4.42, Boundary Loss=0.298, precision_interior=1.01, precision_boundary=1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 0.5360662937164307\n",
      "Switching to L-BFGS Optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation L2 Error at epoch 500: 4.052064418792725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "losses, interior_losses, boundary_losses = train(\n",
    "    model,\n",
    "    adam,\n",
    "    lbfgs,\n",
    "    log_var_interior,\n",
    "    log_var_boundary,\n",
    "    epochs = 5000,\n",
    "    switch_epoch= 500,\n",
    "    n_interior = 4000,\n",
    "    n_boundary = 1000,\n",
    "    print_interval = 100,\n",
    "    save_path = 'biharmonic_DRM/biharmonic_model'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a362925",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlosses\u001b[49m.shape \n",
      "\u001b[31mNameError\u001b[39m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "losses.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb4b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
